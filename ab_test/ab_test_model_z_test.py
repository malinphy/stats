# -*- coding: utf-8 -*-
"""ab_test_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nzWerZ7qf4O0TnGAqkCcX1Qa4hZ9b2Mm
"""
## 2-tail z-test
import numpy as np
import pandas as pd 
import statsmodels.stats.api as sms
import scipy.stats as scs
import matplotlib.pyplot as plt
from scipy.stats import norm
plt.style.use('ggplot')

#import data
raw_data = pd.read_csv("https://raw.githubusercontent.com/malinphy/datasets/main/ab_testing/ab_data.csv")
df = raw_data.copy()

print("Number of rows: ", df.shape[0], " Number of columns: ", df.shape[1])
df.head()
print(df.info())

print(df['group'].value_counts())
print(df['landing_page'].value_counts())
# print(df['converted'].value_counts())

## as seen from the group and the landing page number both numbers are different than each other. This means some data points are duplicated.

c1 = df['group']=='control'
c2 = df['landing_page']=='new_page'

c3 = df['group']=='treatment'
c4 = df['landing_page']=='old_page'

drop1 = df[c1 & c2].index

drop2 = df[c3 & c4].index

df = df.drop(drop1)
df = df.drop(drop2).reset_index(drop = True)
print(df.info())

print(df['group'].value_counts())
print(df['landing_page'].value_counts())

## For this dataset two sample Z-test will be applied. Because number of samples
## are high enough to approach normal distribution
## For this kind of test one can either use proportion of click through rate (CTR) or 
## mean value of the converstion rates 
## In this example I will follow the proportions 

##  As the first step of our test we need to define null and altenative hypothesis 
## our null hypothesis is

"""our null hypothesis is 
$H_0 : p_{con} = p_{exp}$
our alternate hypothesis is 
$H_1 : p_{con} \neq p_{exp}$

$p_{con} = \frac{X_{con}}{N_{con}}$ , 
$p_{exp} = \frac{X_{exp}}{N_{exp}}$
"""

df.head(3)

print(df['converted'].unique())
### conversion value corresponds to 1 therefore if we aggregate sum() function with groupby 
### we can achieve the total number of conversions
control_conversion_number = df.groupby(['group'])['converted'].sum()[0]
treatment_conversion_number = df.groupby(['group'])['converted'].sum()[1]

print('control_conversion_number', control_conversion_number)
print('treatment_conversion_number', treatment_conversion_number)

control_number = df.groupby(['group'])['converted'].count()[0]
treatment_number = df.groupby(['group'])['converted'].count()[1]

print('total number of control group :', control_number )
print('total number of treatment group :', treatment_number )

p_con = control_conversion_number/ control_number
p_trt = treatment_conversion_number/ treatment_number

print('control conversion probability :', p_con)
print('treatment conversion probability :', p_trt)

"""As next step one can calculate the pooled probability using following formula:
$p_{pool} = \frac{X_{con}+X_{exp}}{N_{con}+N_{exp}}$

and pooled variance :
$σ^2_{pool} = p_{pool}(1-p_{pool})(\frac{1}{N_{con}} +\frac{1}{N_{exp}})$ \\
and standard error : $\sigma = \sqrt{σ^2_{pool} }$
"""

p_pool = (control_conversion_number +treatment_conversion_number )/(control_number+treatment_number)

print('pooled probability', p_pool)

pool_var = p_pool*(1-p_pool)*( (1/control_number) + (1/treatment_number ))
print('pooled variance', pool_var)

std_err = np.sqrt(pool_var)

print('standard error:', std_err)

test_stat = (p_con - p_trt) /(std_err)
print('test statistics :', test_stat)
p_value = norm.sf(test_stat)*2

print('p_value', p_value)

alpha  = 0.05 
critical_value = 1.96 ### 
conf_int = [(p_con-p_trt)-critical_value*std_err, (p_con-p_trt)+critical_value*std_err]

print('confidence interval', conf_int)

if np.abs(test_stat) >= critical_value:
    print("reject the null")
    print(p_value)

else :
    print('do not reject the null hypothesis')

